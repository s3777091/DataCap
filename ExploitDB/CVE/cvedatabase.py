import json
import threading
from queue import Queue
import sys
import requests
from bs4 import BeautifulSoup
from time import sleep
from cvetool import extract_code_by_id, extract_detection_methods, extract_text_by_priority

def generateUrls(start, end):
    return [f'https://cwe.mitre.org/data/definitions/{i}.html' for i in range(start, end + 1)]

def checkData(max_cwe):
    try:
        with open('../data/rr.json', 'r') as file:
            existing_data = json.load(file)
            existing_links = [item['url'] for item in existing_data if 'url' in item]
            existing_numbers = [int(link.split('/')[-1].replace('.html', '')) for link in existing_links]
            if existing_numbers and max(existing_numbers) >= max_cwe:
                print(f"Data for CWEs up to {max_cwe} already exists in rr.json.")
                return True
    except (FileNotFoundError, json.JSONDecodeError):
        pass
    return False

def fetchParse(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
    }
    for attempt in range(3):  # Try up to three times
        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 404:
                print(f"404 error for {url}, retrying...")
                continue  # Retry if 404 encountered
            response.raise_for_status()  # Raises an HTTPError if the response was an error
            return BeautifulSoup(response.content, 'html.parser')
        except requests.RequestException as e:
            print(f"Request error: {e}. Retrying ({attempt + 1}/3)...")
            sleep(2 ** attempt)  # Exponential backoff
    return None

skipped_urls = []

def process_url(url, queue):
    global skipped_urls
    cwe_number = url.split('/')[-1].replace('.html', '')
    soup = fetchParse(url)
    if soup:
        examples = extract_code_by_id(soup, f'oc_{cwe_number}_Demonstrative_Examples')

        if examples and all(isinstance(example, dict) for example in examples):
            example_texts = [example.get('code', '') for example in examples if 'code' in example]
            instruction = "What is code vulnerable of this code:\n1. Line of code vulnerable:\n2. Code vulnerable detail:\n"
            data = {
                'instruction': instruction + '\n\n'.join(example_texts),
                'output': ""
            }
            queue.put(data)
        else:
            skipped_urls.append(url)
            print(f"No code examples found, skipped {url}.")
    else:
        skipped_urls.append(url)
        print(f"Skipping {url} due to missing content or repeated errors.")

def main(start, end):
    global skipped_urls
    if checkData(end):
        return

    links = generateUrls(start, end)
    queue = Queue()
    threads = []

    for url in links:
        thread = threading.Thread(target=process_url, args=(url, queue))
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()

    results = []
    while not queue.empty():
        results.append(queue.get())

    with open('../data/rr.json', 'w') as file:
        json.dump(results, file, indent=4)

    if skipped_urls:
        print("The following URLs were skipped due to missing content or errors:")
        for url in skipped_urls:
            print(url)

    print(f"Data for CWEs {start}-{end} has been exported to rr.json successfully.")

if __name__ == "__main__":
    if len(sys.argv) != 2 or '-' not in sys.argv[1]:
        print("Usage: python main.py <start>-<end>")
        sys.exit(1)

    start_end = sys.argv[1].split('-')
    start, end = int(start_end[0]), int(start_end[1])
    main(start, end)
