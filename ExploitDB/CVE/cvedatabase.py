import json
import threading
from queue import Queue
import sys
import requests
from bs4 import BeautifulSoup
from time import sleep
from cvetool import extract_code_by_id, extract_detection_methods, extract_text_by_priority


def generateUrls(start, end):
    return [f'https://cwe.mitre.org/data/definitions/{i}.html' for i in range(start, end + 1)]


def checkData(max_cwe):
    try:
        with open('../data/rr.json', 'r') as file:
            existing_data = json.load(file)
            existing_links = [item['url'] for item in existing_data if 'url' in item]
            existing_numbers = [int(link.split('/')[-1].replace('.html', '')) for link in existing_links]
            if existing_numbers and max(existing_numbers) >= max_cwe:
                print(f"Data for CWEs up to {max_cwe} already exists in rr.json.")
                return True
    except (FileNotFoundError, json.JSONDecodeError):
        pass
    return False


def fetchParse(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
    }
    for attempt in range(3):  # Try up to three times
        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 404:
                print(f"404 error for {url}, retrying...")
                continue  # Retry if 404 encountered
            response.raise_for_status()  # Raises an HTTPError if the response was an error
            return BeautifulSoup(response.content, 'html.parser')
        except requests.RequestException as e:
            print(f"Request error: {e}. Retrying ({attempt + 1}/3)...")
            sleep(2 ** attempt)  # Exponential backoff
    return None


skipped_urls = []


def process_url(url, queue):
    global skipped_urls
    cwe_number = url.split('/')[-1].replace('.html', '')
    soup = fetchParse(url)
    if soup:
        title = soup.find('h2').text.strip() if soup.find('h2') else 'No title'
        description_or_summary = extract_text_by_priority(soup, cwe_number)
        description = extract_text_by_priority(soup, cwe_number)
        mitigation = extract_text_by_priority(soup, cwe_number)

        detection_methods = extract_detection_methods(soup, f'oc_{cwe_number}_Detection_Methods')
        examples = extract_code_by_id(soup, f'oc_{cwe_number}_Demonstrative_Examples')

        instruction = f"Learning from {title}"
        output_parts = []

        if description_or_summary:
            output_parts.append(f"short description or summary: {description_or_summary}")
        if description:
            output_parts.append(f"extended description: {description}")
        if detection_methods:
            output_parts.append(f"detection methods: {detection_methods}")
        if examples:
            output_parts.append(f"examples code: {examples}")
        if mitigation:
            output_parts.append(f"Potential Mitigations: {mitigation}")

        output = "\n".join(output_parts)

        data = {
            'instruction': instruction,
            'output': output
        }
        queue.put(data)
    else:
        skipped_urls.append(url)
        print(f"Skipping {url} due to missing content or repeated errors.")


def main(start, end):
    global skipped_urls
    if checkData(end):
        return

    links = generateUrls(start, end)
    queue = Queue()
    threads = []

    for url in links:
        thread = threading.Thread(target=process_url, args=(url, queue))
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()

    results = []
    while not queue.empty():
        results.append(queue.get())

    with open('../data/rr.json', 'w') as file:
        json.dump(results, file, indent=4)

    if skipped_urls:
        print("The following URLs were skipped due to missing content or errors:")
        for url in skipped_urls:
            print(url)

    print(f"Data for CWEs {start}-{end} has been exported to rr.json successfully.")


if __name__ == "__main__":
    main(500, 1000)