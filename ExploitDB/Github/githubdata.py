import requests
from bs4 import BeautifulSoup
import json
import concurrent.futures

def get_advisory_links(page):
    url = f'https://github.com/advisories?page={page}'
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    links = [a['href'] for a in soup.find_all('a', href=True) if a['href'].startswith('/advisories/')]
    return links

def get_code_from_advisory(link):
    url = f'https://github.com{link}'
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    code_classes = [
        'highlight highlight-source-js',
        'highlight highlight-source-erlang',
        'highlight highlight-source-go',
        'highlight highlight-source-python',
        'highlight highlight-source-ruby',
        'highlight highlight-source-yaml',
        'highlight highlight-source-swift'
    ]

    for code_class in code_classes:
        code_block = soup.find_all('div', class_=code_class)
        all_codes = [block.get_text() for block in code_block]
        if all_codes:
            instruction = "What is code vulnerable of this code:\n1. Line of code vulnerable:\n2. Code vulnerable detail:\n"
            return {
                "instruction": instruction + '\n\n'.join(all_codes),
                "output": ""
            }
    return None

def process_page(page):
    results = []
    print(f"Processing page {page}...")
    links = get_advisory_links(page)
    for link in links:
        code_entry = get_code_from_advisory(link)
        if code_entry:
            results.append(code_entry)

    return results

def main():
    pages = range(1, 24)
    results = []
    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = [executor.submit(process_page, page) for page in pages]
        for future in concurrent.futures.as_completed(futures):
            results.extend(future.result())

    with open('../data/vulnerable_codes.json', 'w') as f:
        json.dump(results, f, indent=4)

if __name__ == "__main__":
    main()
