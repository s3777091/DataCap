import requests
import json
import os
from concurrent.futures import ThreadPoolExecutor, as_completed

def fetch(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/58.0.3029.110 Safari/537.36',
        'Accept': 'text/plain',
    }
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        return {"content": response.text}
    else:
        print(f"Failed to retrieve the webpage: Status code {response.status_code}")
        return {"content": ""}

def parse(pr):
    lines = pr["content"].split('\n')
    dict = {
        "title": "",
        "role": "user",
        "content": "what is code vulnerable of this code:\n1. Line of code vulnerable:\n2. code vulnerable detail:\n"
    }
    for line in lines:
        if '# Title:' in line or '# Exploit Title:' in line:
            dict["title"] = line.split(':', 1)[1].strip() if len(line.split(':', 1)) > 1 else ""
        elif not line.startswith('#') and line.strip():
            dict["content"] += line.strip() + "\n"
    return dict

def SaveContent(start_id, count):
    base_url = "https://www.exploit-db.com/raw/"
    urls = [f"{base_url}{start_id - i}" for i in range(count)]
    data = []

    with ThreadPoolExecutor(max_workers=10) as executor:
        future_to_url = {executor.submit(fetch, url): url for url in urls}
        for future in as_completed(future_to_url):
            content = future.result()
            data.append(parse(content))

    os.makedirs("data", exist_ok=True)
    file_name = f"{start_id}-{start_id - count + 1}.json"

    with open(os.path.join("data", file_name), 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 3:
        print("Usage: python main.py [start_id] [count]")
        sys.exit(1)

    start_id = int(sys.argv[1])
    count = int(sys.argv[2])
    SaveContent(start_id, count)